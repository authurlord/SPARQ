{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 这个notebook聚焦于 Ablation Study，针对 Wikitq 的 Test\n",
    "from utils.schedule_utils import load_data_split,table_to_str,table_to_str_sql,find_intersection_and_add_row_id,Prepare_Data_for_Operator_Sequence,format_document,batch_rerank_scores,ROLLBACK,merge_clean_and_format_df_dict,retrieve_rows_by_subtables,process_error_analysis_list\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.prompt_generate import build_wikitq_prompt_from_df,evaluate_predictions,filter_dataframe_from_responses,fix_sql_query,match_subtables,retrieve_rows_by_subtables,build_tab_fact_prompt_from_df\n",
    "from utils.async_llm import infer_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取必要文件 （step 2）\n",
    "dataset_name = 'wikitq'\n",
    "split = 'test'\n",
    "dataset = load_data_split(dataset_name,split)\n",
    "wikitq_df_processed = np.load(f'datasets/schedule_test/{dataset_name}/wikitq_df_processed.npy',allow_pickle=True).item()\n",
    "# wikitq_df_processed = np.load('/data/workspace/yanmy/HybridRAG/H-STAR/datasets/wikitq_df_validation.npy', allow_pickle=True).item()\n",
    "assert len(dataset) == len(wikitq_df_processed)\n",
    "wikitq_df = pd.DataFrame(dataset)\n",
    "long_index = np.load('../datasets/pipeline/valid_data_selection.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c841fd9ce949f29ca8dca54d3cc734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Samples: 80; Total Samples: 100\n",
      "Accuracy: 80.00\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv('../datasets/pipeline/wikitq/wikitq_test_rewrite_qa_output_4B.csv',index_col=0)\n",
    "_,_,_ = evaluate_predictions(dataset_name,result.iloc[:100],dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert on table data.\n",
      "You must use the table data and the additional evidence to answer the given question.\n",
      "\n",
      "Procedure:\n",
      "- Divide the main statement into sub-tasks and answer each sub-task\n",
      "- Based on the answers, check whether the statement is supported by the table\n",
      "\n",
      "**NOTE** \n",
      "Please be extremely careful, pause, make sure all instructions have been followed and only then output the answer\n",
      "\n",
      "Response Format:\n",
      "Begin your response with 'Output: ' and always include the following:\n",
      "- Decompose: Divide the main question into sub-tasks and answer each sub-task\n",
      "- Final Answer: Strictly output as a short phrase starting by `therefore, the answer is: \"AnswerName1\", \"AnswerName2\"...` form, no other form\n",
      "\n",
      "- Read the question carefully, understand, and return what the question asks.\n",
      "- Be careful, make sure you have followed all instructions and only then return the output.\n",
      "```\n",
      "<input>\n",
      "table caption: List of spans\n",
      "/*\n",
      "col : tramway | year of inauguration\n",
      "row 0 : 3s aerial tramway | 2004\n",
      "row 1 : sandia peak tramway\t| 1966\n",
      "*/\n",
      "columns: ['tramway', 'year of inauguration']\n",
      "Q: was the sandia peak tramway innagurate before or after the 3s aerial tramway?\n",
      "<output>\n",
      "Use the table to answer the question.\n",
      "1. Decompose:\n",
      "    - #1: inauguration year of sandia peak tramway = 1966\n",
      "    - #2: inauguration year of 3s aerial tramway = 2004\n",
      "    - #3: #1 is before #2\n",
      "2. Final Answer: Therefore, the answer is: \"before\"\n",
      "\n",
      "<input>\n",
      "/*\n",
      "col : rank | cyclist | team\n",
      "row 0 : alejandro valverde (esp) | caisse d'epargne\n",
      "row 1 : alexandr kolobnev (rus) | team csc saxo bank\n",
      "row 2 : davide rebellin (ita) | gerolsteiner\n",
      "row 3 : paolo bettini (ita) | quick step\n",
      "row 4 : franco pellizotti (ita) | liquigas\n",
      "row 5 : denis menchov (rus) | rabobank\n",
      "row 6 : samuel sánchez (esp) | euskaltel-euskadi\n",
      "row 7 : stéphane goubert (fra) | ag2r-la mondiale\n",
      "row 8 : haimar zubeldia (esp) | euskaltel-euskadi\n",
      "row 9 : david moncoutié (fra) | cofidis\n",
      "*/\n",
      "columns: ['rank', 'cyclist', 'team']\n",
      "Q: which country had the most cyclists finish within the top 10?\n",
      "<output>\n",
      "\n",
      "Here is an additional evidence to help the answering process.\n",
      "Additional Evidence:\n",
      "/*\n",
      "col : country | total_cyclists_per_country\n",
      "row 0 : ita | 3\n",
      "*/\n",
      "Using the table and the additional evidence to answer the question\n",
      "1. Decompose:\n",
      "    - #1: Number of cyclists from Italy in the top 10 = 3\n",
      "2. Final Answer: Therefore, the answer is: \"Italy\"\n",
      "\n",
      "<input>\n",
      "table caption: Matthew Morrison\n",
      "/*\n",
      "col : year | title\n",
      "row 0 :\t2007 | music and lyrics\n",
      "row 1 :\t2007 | dan in real life\n",
      "row 2 :\t2007 | i think i love my wife\n",
      "*/\n",
      "columns: ['year', 'title']\n",
      "Q: what movies other than 'music and lyrics' was morrison involved with in 2007?\n",
      "<output>\n",
      "1. Decompose:\n",
      "    - #1: Movies Matthew Morrison was involved with in 2007 apart from 'music and lyrics':\n",
      "        - dan in real life\n",
      "        - i think i love my wife\n",
      "2. Final Answer: Therefore, the answer is: \"dan in real life\", \"i think i love my wife\"\n",
      "\n",
      "<input>\n",
      "table caption: 2007 New Orleans Saints season\n",
      "/*\n",
      "col : game site\t| result/score\n",
      "row 0 : rca dome | l 41 – 10\n",
      "row 1 :\traymond james stadium | l 31 – 14\n",
      "row 2 :\tlouisiana superdome | l 31 – 14\n",
      "row 4 :\tlouisiana superdome\t| l 16 – 13\n",
      "row 9 :\tlouisiana superdome\t| l 37 – 29\n",
      "row 10 : reliant stadium | l 23 – 10\n",
      "row 12 : louisiana superdome | l 27 – 23\n",
      "row 15 : louisiana superdome | l 38–23\n",
      "row 16 : soldier field | l 33–25\n",
      "*/\n",
      "columns: ['game site', 'result/score']\n",
      "Q: what number of games were lost at home?\n",
      "<output>\n",
      "\n",
      "Here is an additional evidence to help the answering proces\n",
      "Additional Evidence:\n",
      "/*\n",
      "col : games_lost_at_home\n",
      "row 0 : 5\n",
      "*/\n",
      "Using the table and the additional evidence to answer the question\n",
      "1. Decompose:\n",
      "    - #1: From the additional evidence, number of games lost at home = 5\n",
      "          From the table, counting the occurrences of \"louisiana superdome\" in the \"game site\" and 'result/score' for loss column = 5\n",
      "2. Final Answer: Therefore, the answer is: \"5\"\n",
      "```\n",
      "<input>\n",
      "table caption: Jan Kudlička\n",
      "/*\n",
      "col : year | competition | venue | position | notes\n",
      "row 0 : 2005 | world youth championships | marrakech, morocco | 6th | 5.05 m\n",
      "row 1 : 2006 | world junior championships | beijing, china | 5th | 5.3 m\n",
      "row 2 : 2008 | olympic games | beijing, china | 10th | 5.45 m\n",
      "row 3 : 2009 | european u23 championships | kaunas, lithuania | 8th | 5.15 m\n",
      "row 4 : 2009 | world championships | berlin, germany | 22nd (q) | 5.4 m\n",
      "row 5 : 2010 | european championships | barcelona, spain | 10th | 5.6 m\n",
      "row 6 : 2011 | world championships | daegu, south korea | 9th | 5.65 m\n",
      "row 7 : 2012 | european championships | helsinki, finland | 6th | 5.6 m\n",
      "row 8 : 2012 | olympic games | london, united kingdom | 8th | 5.65 m\n",
      "row 9 : 2013 | european indoor championships | gothenburg, sweden | 5th | 5.71 m\n",
      "row 10 : 2014 | world indoor championships | sopot, poland | 3rd | 5.8 m\n",
      "*/\n",
      "columns: ['year', 'competition', 'venue', 'position', 'notes']\n",
      "Q: [ORIGINAL]: which competition was held in berlin and daegu?  \n",
      "[GENERAL]: Identify the competitions that took place in Berlin, Germany and Daegu, South Korea based on the venue column.  \n",
      "[BALANCED]: Find the competition names where the venue is either 'berlin, germany' or 'daegu, south korea' using the 'competition' and 'venue' columns.  \n",
      "[SPECIFIC]: competition venue berlin germany daegu south korea world championships european championships world championships european championships\n",
      "<output>\n",
      "Here is an additional evidence to help the answering process.\n",
      "Additional Evidence:\n",
      "/*\n",
      "col : row_id | competition\n",
      "row 0 : 4 | world championships\n",
      "row 1 : 6 | world championships\n",
      "*/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.iloc[55,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate training-free router result\n",
    "prompt_list = []\n",
    "for index in range(len(dataset)):\n",
    "    index = int(index)\n",
    "\n",
    "    prompt = build_wikitq_prompt_from_df(dataset,wikitq_df_processed[index],index,template_path='../prompts/training_free_router.txt',processed=True)\n",
    "    prompt_list.append(prompt)\n",
    "wikitq_df_rewrite_qa_semantic = wikitq_df\n",
    "wikitq_df_rewrite_qa_semantic['instruction'] = prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitq_df_rewrite_qa_semantic.to_csv('../datasets/ablation/wikitq_test_training_free_router.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hstar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
