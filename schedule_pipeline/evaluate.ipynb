{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.schedule_utils import load_data_split,table_to_str,table_to_str_sql,find_intersection_and_add_row_id,Prepare_Data_for_Operator_Sequence,format_document,batch_rerank_scores,ROLLBACK,merge_clean_and_format_df_dict,retrieve_rows_by_subtables,process_error_analysis_list\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.prompt_generate import build_wikitq_prompt_from_df,evaluate_predictions,filter_dataframe_from_responses,fix_sql_query,match_subtables,retrieve_rows_by_subtables,build_tab_fact_prompt_from_df\n",
    "from utils.async_llm import infer_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取必要文件 （step 2）\n",
    "dataset_name = 'wikitq'\n",
    "split = 'test'\n",
    "dataset = load_data_split(dataset_name,split)\n",
    "wikitq_df_processed = np.load(f'datasets/schedule_test/{dataset_name}/wikitq_df_processed.npy',allow_pickle=True).item()\n",
    "wikitq_df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29754a24c574bfdaec085af7d88e1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Samples: 2976; Total Samples: 4344\n",
      "Accuracy: 68.51\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(f'../datasets/pipeline/wikitq/wikitq_test_rewrite_RAG_qa_output_4B.csv',index_col=0)\n",
    "acc_all, error_index_all, format_error_index_all = evaluate_predictions(dataset_name, result, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Ablation Test for Query Rewrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = []\n",
    "for index in range(len(dataset)):\n",
    "    prompt = build_wikitq_prompt_from_df(dataset,wikitq_df_processed[index],index,template_path='../prompts/rewrite_query.txt',processed=True)\n",
    "    prompt_list.append(prompt)\n",
    "wikitq_df['instruction'] = prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitq_df.to_csv('../datasets/pipeline/wikitq/wikitq_test_rewrite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_result = pd.read_csv('../datasets/pipeline/wikitq/wikitq_test_rewrite_output_4B.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the number of 1st place finishes across all events?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[GENERAL]: Count the total number of times a rider achieved a 1st place finish (placing = 1) across all events and competitions.  \\n[BALANCED]: Count rows where 'placing' is '1' in the 'placing' column to determine the total number of first-place finishes.  \\n[SPECIFIC]: count first place finishes placing 1 event competition location country rider nationality victoria pendleton gbr jason kenny gbr ross edgar gbr jamie staff gbr chris hoy gbr\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../schedule_pipeline/datasets/schedule_test/wikitq/rewrite_query_4B.npy',rewrite_query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python Hybrid_Retrieve_Update.py --model_path /data/workspace/yanmy/models/bge-m3 --dataset_name wikitq --split test --index_path datasets/schedule_test/wikitq/RAG_index_all.npy --output_path datasets/schedule_test/wikitq/Hybrid_Retrieve_output_update.npy --max_rows 50 --max_cols 10 --processed_df_path datasets/schedule_test/wikitq_7B/wikitq_df_processed.npy --rewrite_query_path datasets/schedule_test/wikitq/rewrite_query_4B.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/schedule_test/wikitq/RAG_index_all.npy',[i for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG = np.load('datasets/schedule_test/wikitq/Hybrid_Retrieve_output_update.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_query_list = []\n",
    "for index in range(len(dataset)):\n",
    "    rewritten_query = eval(rewrite_result.iloc[index,-1])[0]\n",
    "    # dataset[index]['question'] = rewritten_query\n",
    "    rewrite_query_list.append(rewritten_query)\n",
    "    # wikitq_df.at[index,'rewritten_query'] = rewritten_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_table = np.load('../datasets/check/wikitq_test_processed_table.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = []\n",
    "for index in range(len(dataset)):\n",
    "    rewritten_query = eval(rewrite_result.iloc[index,-1])[0]\n",
    "    # sequence = Check_Model_Data_Sequence[index]['Sequence']\n",
    "    prompt = build_wikitq_prompt_from_df(dataset,RAG[index],index,template_path='../prompts/text_reason_wtq.txt',processed=True)\n",
    "    prompt = prompt.replace(dataset[index]['question'], f\"[ORIGINAL]: {dataset[index]['question']}  \\n\" + rewritten_query)\n",
    "    # if sequence==[] or sequence.__contains__('Execute_SQL'):\n",
    "    # evidence = table_to_str_sql(processed_table['Execute_SQL'][index])\n",
    "    # prompt = prompt + evidence\n",
    "    prompt_list.append(prompt)\n",
    "wikitq_df['instruction'] = prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitq_df.to_csv('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(dataset_name, df, dataset):\n",
    "    acc =0\n",
    "    error_index = []\n",
    "    format_error_index = []\n",
    "    # dataset_name = 'wikitq'\n",
    "    dataset_df_output = df\n",
    "    outputs = dataset_df_output.to_dict(orient='index')\n",
    "    # Iterate through each output and calculate the accuracy\n",
    "    count = 0\n",
    "    for i in tqdm(outputs.keys()):\n",
    "        count += 1\n",
    "        output = outputs[i]\n",
    "        pred_answer = None\n",
    "\n",
    "        # --- Prediction Parsing Logic ---\n",
    "        if dataset_name == 'wikitq':\n",
    "            try:\n",
    "                # Attempt to parse different possible output formats\n",
    "                raw_output = eval(output['predict'])[0]\n",
    "                if 'the answer is: ' in raw_output:\n",
    "                    pred_answer_all = raw_output.split('the answer is: ')[1]\n",
    "                elif 'Final Answer: ' in raw_output:\n",
    "                    pred_answer_all = raw_output.split('Final Answer: ')[1]\n",
    "                else:\n",
    "                    raise ValueError(\"Unrecognized format\")\n",
    "                \n",
    "                # Extract the answer, which is often quoted\n",
    "                pred_answer = pred_answer_all.split('\"')[1:2]\n",
    "\n",
    "            except Exception as e:\n",
    "                # If parsing fails, log it as a format error\n",
    "                # print(f'Format error in index {i}: {e}')\n",
    "                format_error_index.append(i)\n",
    "                pred_answer = [''] # Assign an empty prediction\n",
    "                pred_answer_all = ''\n",
    "                # print(pred_answer)\n",
    "\n",
    "        elif dataset_name == 'tab_fact':\n",
    "            # For TabFact, check for 'true' or 'false' (case-insensitive)\n",
    "            # if isinstance(output['predict'], str) and 'true' in output['predict'].lower():\n",
    "            try:\n",
    "                pred_answer_str = eval(output['predict'])[0].lower().split('statement is: ')[1].replace(\" \", \"\")\n",
    "            except:\n",
    "                pred_answer_str = eval(output['predict'])[0]\n",
    "            if 'false' in pred_answer_str:\n",
    "                pred_answer = [0]\n",
    "            else:\n",
    "                pred_answer = [1]\n",
    "\n",
    "            # gold_answer = result_df[str(i)]['ori_data_item']['answer_text']\n",
    "        gold_answer = dataset[i]['answer_text']\n",
    "        # Score is either 1 or 0\n",
    "        # print(pred_answer)\n",
    "        score = Evaluator().evaluate(\n",
    "        pred_answer,\n",
    "        gold_answer,\n",
    "        dataset=dataset_name,\n",
    "        question=output['question']\n",
    "        )\n",
    "        if dataset_name=='wikitq':\n",
    "            if score == False and isinstance(pred_answer_all, str):\n",
    "                score = Evaluator().evaluate(\n",
    "                    pred_answer_all.split(','),\n",
    "                    gold_answer,\n",
    "                    dataset=dataset_name,\n",
    "                    question=output.get('question', '') # Safely get question if available\n",
    "                )\n",
    "        acc += score\n",
    "        if score != 1:\n",
    "            error_index.append(i)\n",
    "            # if score!=1:\n",
    "            #     print(f'The prediction is {pred_answer}, while the ground truth is {gold_answer} for sample {i}.')\n",
    "        # except:\n",
    "        #     print(i)\n",
    "        #     pass\n",
    "    final_accuracy = 100 * acc / count if count > 0 else 0\n",
    "    print(f\"Correct Samples: {acc}; Total Samples: {count}\")\n",
    "    print(f\"Accuracy: {100*acc/count:.2f}\")\n",
    "    return final_accuracy, error_index, format_error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikitq'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../datasets/pipeline/wikitq/wikitq_test_rewrite_qa_output_4B.csv',index_col=0)\n",
    "acc_all, error_index_all, format_error_index_all = evaluate_predictions(dataset_name, result, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4344/4344 [01:23<00:00, 51.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Samples: 3231; Total Samples: 4344\n",
      "Accuracy: 74.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv('../datasets/wikitq_test_output_4B.csv',index_col=0)\n",
    "acc_all, error_index_all, format_error_index_all = evaluate_predictions(dataset_name, result, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4344/4344 [01:21<00:00, 53.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Samples: 3312; Total Samples: 4344\n",
      "Accuracy: 76.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv('../datasets/pipeline/wikitq/wikitq_test_rewrite_qa_output_4B.csv',index_col=0)\n",
    "acc_all, error_index_all, format_error_index_all = evaluate_predictions(dataset_name, result, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert on table data.\n",
      "You must use the table data and the additional evidence to answer the given question.\n",
      "\n",
      "Procedure:\n",
      "- Divide the main statement into sub-tasks and answer each sub-task\n",
      "- Based on the answers, check whether the statement is supported by the table\n",
      "\n",
      "**NOTE** \n",
      "Please be extremely careful, pause, make sure all instructions have been followed and only then output the answer\n",
      "\n",
      "Response Format:\n",
      "Begin your response with 'Output: ' and always include the following:\n",
      "- Decompose: Divide the main question into sub-tasks and answer each sub-task\n",
      "- Final Answer: Strictly output as a short phrase starting by `therefore, the answer is: \"AnswerName1\", \"AnswerName2\"...` form, no other form\n",
      "\n",
      "- Read the question carefully, understand, and return what the question asks.\n",
      "- Be careful, make sure you have followed all instructions and only then return the output.\n",
      "```\n",
      "<input>\n",
      "table caption: List of spans\n",
      "/*\n",
      "col : tramway | year of inauguration\n",
      "row 0 : 3s aerial tramway | 2004\n",
      "row 1 : sandia peak tramway\t| 1966\n",
      "*/\n",
      "columns: ['tramway', 'year of inauguration']\n",
      "Q: was the sandia peak tramway innagurate before or after the 3s aerial tramway?\n",
      "<output>\n",
      "Use the table to answer the question.\n",
      "1. Decompose:\n",
      "    - #1: inauguration year of sandia peak tramway = 1966\n",
      "    - #2: inauguration year of 3s aerial tramway = 2004\n",
      "    - #3: #1 is before #2\n",
      "2. Final Answer: Therefore, the answer is: \"before\"\n",
      "\n",
      "<input>\n",
      "/*\n",
      "col : rank | cyclist | team\n",
      "row 0 : alejandro valverde (esp) | caisse d'epargne\n",
      "row 1 : alexandr kolobnev (rus) | team csc saxo bank\n",
      "row 2 : davide rebellin (ita) | gerolsteiner\n",
      "row 3 : paolo bettini (ita) | quick step\n",
      "row 4 : franco pellizotti (ita) | liquigas\n",
      "row 5 : denis menchov (rus) | rabobank\n",
      "row 6 : samuel sánchez (esp) | euskaltel-euskadi\n",
      "row 7 : stéphane goubert (fra) | ag2r-la mondiale\n",
      "row 8 : haimar zubeldia (esp) | euskaltel-euskadi\n",
      "row 9 : david moncoutié (fra) | cofidis\n",
      "*/\n",
      "columns: ['rank', 'cyclist', 'team']\n",
      "Q: which country had the most cyclists finish within the top 10?\n",
      "<output>\n",
      "\n",
      "Here is an additional evidence to help the answering process.\n",
      "Additional Evidence:\n",
      "/*\n",
      "col : country | total_cyclists_per_country\n",
      "row 0 : ita | 3\n",
      "*/\n",
      "Using the table and the additional evidence to answer the question\n",
      "1. Decompose:\n",
      "    - #1: Number of cyclists from Italy in the top 10 = 3\n",
      "2. Final Answer: Therefore, the answer is: \"Italy\"\n",
      "\n",
      "<input>\n",
      "table caption: Matthew Morrison\n",
      "/*\n",
      "col : year | title\n",
      "row 0 :\t2007 | music and lyrics\n",
      "row 1 :\t2007 | dan in real life\n",
      "row 2 :\t2007 | i think i love my wife\n",
      "*/\n",
      "columns: ['year', 'title']\n",
      "Q: what movies other than 'music and lyrics' was morrison involved with in 2007?\n",
      "<output>\n",
      "1. Decompose:\n",
      "    - #1: Movies Matthew Morrison was involved with in 2007 apart from 'music and lyrics':\n",
      "        - dan in real life\n",
      "        - i think i love my wife\n",
      "2. Final Answer: Therefore, the answer is: \"dan in real life\", \"i think i love my wife\"\n",
      "\n",
      "<input>\n",
      "table caption: 2007 New Orleans Saints season\n",
      "/*\n",
      "col : game site\t| result/score\n",
      "row 0 : rca dome | l 41 – 10\n",
      "row 1 :\traymond james stadium | l 31 – 14\n",
      "row 2 :\tlouisiana superdome | l 31 – 14\n",
      "row 4 :\tlouisiana superdome\t| l 16 – 13\n",
      "row 9 :\tlouisiana superdome\t| l 37 – 29\n",
      "row 10 : reliant stadium | l 23 – 10\n",
      "row 12 : louisiana superdome | l 27 – 23\n",
      "row 15 : louisiana superdome | l 38–23\n",
      "row 16 : soldier field | l 33–25\n",
      "*/\n",
      "columns: ['game site', 'result/score']\n",
      "Q: what number of games were lost at home?\n",
      "<output>\n",
      "\n",
      "Here is an additional evidence to help the answering proces\n",
      "Additional Evidence:\n",
      "/*\n",
      "col : games_lost_at_home\n",
      "row 0 : 5\n",
      "*/\n",
      "Using the table and the additional evidence to answer the question\n",
      "1. Decompose:\n",
      "    - #1: From the additional evidence, number of games lost at home = 5\n",
      "          From the table, counting the occurrences of \"louisiana superdome\" in the \"game site\" and 'result/score' for loss column = 5\n",
      "2. Final Answer: Therefore, the answer is: \"5\"\n",
      "```\n",
      "<input>\n",
      "table caption: Super League XVIV\n",
      "/*\n",
      "col : team | stadium | capacity | city/area\n",
      "row 0 : bradford bulls (2014 season) | provident stadium | 27000 | bradford, west yorkshire\n",
      "row 1 : castleford tigers (2014 season) | the wish communications stadium | 11750 | castleford, west yorkshire\n",
      "row 2 : catalans dragons (2014 season) | stade gilbert brutus | 14000 | perpignan, pyrénées-orientales, france\n",
      "row 3 : huddersfield giants (2014 season) | john smith's stadium | 24544 | huddersfield, west yorkshire\n",
      "row 4 : hull (2014 season) | kingston communications stadium | 25404 | kingston upon hull, east riding of yorkshire\n",
      "row 5 : hull kingston rovers (2014 season) | ms3 craven park | 9471 | kingston upon hull, east riding of yorkshire\n",
      "row 6 : leeds rhinos (2014 season) | headingley carnegie stadium | 22250 | leeds, west yorkshire\n",
      "row 7 : london broncos (2014 season) | twickenham stoop | 12700 | twickenham, london\n",
      "row 8 : salford city reds (2014 season) | salford city stadium | 12000 | salford, greater manchester\n",
      "row 9 : st helens rlfc (2014 season) | langtree park | 18000 | st helens, merseyside\n",
      "row 10 : wakefield trinity wildcats (2014 season) | rapid solicitors stadium | 11000 | wakefield, west yorkshire\n",
      "row 11 : warrington wolves (2014 season) | halliwell jones stadium | 15500 | warrington, cheshire\n",
      "row 12 : widnes vikings (2014 season) | the select security stadium | 13500 | widnes, cheshire, england\n",
      "row 13 : wigan warriors (2014 season) | dw stadium | 25138 | wigan, greater manchester\n",
      "*/\n",
      "columns: ['team', 'stadium', 'capacity', 'city/area']\n",
      "Q: [ORIGINAL]: what is the last stadium listed on this chart?  \n",
      "[GENERAL]: Identify the name of the stadium corresponding to the last row in the table when ordered by row index.  \n",
      "[BALANCED]: Find the stadium name from the last row using the 'stadium' column in the table.  \n",
      "[SPECIFIC]: stadium last row team stadium capacity city/area dw stadium wigan warriors 25138 wigan greater manchester\n",
      "<output>\n"
     ]
    }
   ],
   "source": [
    "print(result.iloc[31,-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hstar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
